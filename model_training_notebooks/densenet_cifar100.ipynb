{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ],
   "id": "540f0961ad2ba005"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "id": "d44ff1b4dce76937"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "IMAGE_SIZE = 32\n",
    "mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]\n",
    "# source: https://pytorch.org/vision/stable/transforms.html\n",
    "transforms_train = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean, std)])\n",
    "\n",
    "\n",
    "transforms_test = transforms.Compose([transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean, std)])\n",
    "\n",
    "train_dataset =  datasets.CIFAR100(root='./data', train=True, download=True, transform = transforms_train)\n",
    "validation_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform = transforms_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=512, num_workers=31, persistent_workers=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=512, num_workers=31, persistent_workers=True)"
   ],
   "id": "44d3491179cff6c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# show some train images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def show_img(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "show_img(torchvision.utils.make_grid(images[:8], nrow=4, padding=2))"
   ],
   "id": "81829405034483ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from lit_modules.densenet_lit import DenseNetLit\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ],
   "id": "218ddf4af5738f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(self, train_loader, validation_loader):\n",
    "        super().__init__()\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_loader\n",
    "    \n",
    "\n",
    "data=DataModule(train_loader, validation_loader)\n",
    "hyperparameters = {\n",
    "    \"depth\": 40,\n",
    "    \"first_output\": 16,\n",
    "    \"growth_rate\": 12,\n",
    "    \"dropout\": 0.2,\n",
    "}\n",
    "model = DenseNetLit(hyperparameters=hyperparameters)\n",
    "logger = MLFlowLogger(experiment_name=\"DenseNet\", save_dir=\"mlruns\")\n",
    "trainer = L.Trainer(max_epochs=50, logger=logger, callbacks=\n",
    "                    [ModelCheckpoint(monitor=\"val_f1_macro\", mode=\"max\", dirpath=\"checkpoints/densenet\", filename=\"{epoch:02d}-{val_f1_macro:.3f}-{val_accuracy:.3f}\")],\n",
    "                    precision=\"16-mixed\", \n",
    "                    num_sanity_val_steps=0)\n",
    "trainer.fit(model, datamodule=data)"
   ],
   "id": "4fdb9150d6ca7756"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "At epoch 48 validation f1 macro score is 0.89 while training with max_epochs=50",
   "id": "b1160c893a4fc303"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualization of the training process - 50 epochs\n",
    "\n",
    "![Learning Rate](../img/lr_epoch.png)\n",
    "![Train Loss](../img/train_loss_epoch.png)\n",
    "![Validation Loss](../img/val_loss_epoch.png)"
   ],
   "id": "3c64df0b6f0d0bf6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fc3ec5db3c3390a7"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}

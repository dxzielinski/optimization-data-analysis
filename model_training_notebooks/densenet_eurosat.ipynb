{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8342a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from lit_modules.densenet_lit import DenseNetLit\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "L.seed_everything(42)\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]\n",
    "# source: https://pytorch.org/vision/stable/transforms.html\n",
    "transforms_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "transforms_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "whole_dataset = datasets.EuroSAT(\n",
    "    root=\"./data\", download=True, transform=transforms_train\n",
    ")\n",
    "N = len(whole_dataset)\n",
    "num_val = int(0.2 * N)\n",
    "num_test = int(0.2 * N)\n",
    "num_train = N - num_val - num_test\n",
    "all_indices = list(range(N))\n",
    "all_indices = torch.randperm(N).tolist()\n",
    "indices_val = torch.randperm(N)[:num_val]\n",
    "indices_test = torch.randperm(N)[:num_test]\n",
    "train_indices = all_indices[:num_train]\n",
    "val_indices = all_indices[num_train : num_train + num_val]\n",
    "test_indices = all_indices[num_train + num_val : num_train + num_val + num_test]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(whole_dataset, train_indices)\n",
    "validation_dataset = torch.utils.data.Subset(whole_dataset, val_indices)\n",
    "test_dataset = torch.utils.data.Subset(whole_dataset, test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=512,\n",
    "    num_workers=30,\n",
    "    persistent_workers=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset=validation_dataset, batch_size=512, num_workers=30, persistent_workers=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=512, num_workers=30, persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "373cb375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.6719378..2.0591094].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_img(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "show_img(torchvision.utils.make_grid(images[:8], nrow=4, padding=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae9c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model         | DenseNet         | 1.0 M  | train\n",
      "1 | loss_fn       | CrossEntropyLoss | 0      | train\n",
      "2 | train_metrics | MetricCollection | 0      | train\n",
      "3 | val_metrics   | MetricCollection | 0      | train\n",
      "4 | test_metrics  | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.079     Total estimated model params size (MB)\n",
      "256       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/32 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 7.63 GiB of which 84.81 MiB is free. Process 7416 has 1.40 GiB memory in use. Process 10037 has 186.00 MiB memory in use. Including non-PyTorch memory, this process has 5.03 GiB memory in use. Of the allocated memory 4.76 GiB is allocated by PyTorch, and 128.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     26\u001b[39m logger = MLFlowLogger(experiment_name=\u001b[33m\"\u001b[39m\u001b[33mDensenet-EuroSat\u001b[39m\u001b[33m\"\u001b[39m, save_dir=\u001b[33m\"\u001b[39m\u001b[33mmlruns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m trainer = L.Trainer(\n\u001b[32m     28\u001b[39m     max_epochs=\u001b[32m50\u001b[39m,\n\u001b[32m     29\u001b[39m     logger=logger,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     num_sanity_val_steps=\u001b[32m0\u001b[39m,\n\u001b[32m     40\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    322\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    179\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/core/module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1272\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1273\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1276\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1277\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/plugins/precision/amp.py:79\u001b[39m, in \u001b[36mMixedPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n\u001b[32m     82\u001b[39m skip_unscaling = closure_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model.automatic_optimization\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m         \u001b[38;5;28mself\u001b[39m.warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/optimization/automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \n\u001b[32m    316\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer.world_size > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/lit_modules/densenet_lit.py:70\u001b[39m, in \u001b[36mDenseNetLit.training_step\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[32m     69\u001b[39m     x, y = batch\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.loss_fn(logits, y)\n\u001b[32m     72\u001b[39m     probabilities = torch.softmax(logits, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/lit_modules/densenet_lit.py:66\u001b[39m, in \u001b[36mDenseNetLit.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/models/densenet.py:117\u001b[39m, in \u001b[36mDenseNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    116\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.conv1(x)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.trans1(x)\n\u001b[32m    119\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.block2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/models/densenet.py:47\u001b[39m, in \u001b[36mDenseLayer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbranch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([x, out], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2822\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2820\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 7.63 GiB of which 84.81 MiB is free. Process 7416 has 1.40 GiB memory in use. Process 10037 has 186.00 MiB memory in use. Including non-PyTorch memory, this process has 5.03 GiB memory in use. Of the allocated memory 4.76 GiB is allocated by PyTorch, and 128.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(self, train_loader, validation_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    "\n",
    "data = DataModule(train_loader, validation_loader, test_loader)\n",
    "hyperparameters = {\n",
    "    \"depth\": 40,\n",
    "    \"first_output\": 16,\n",
    "    \"growth_rate\": 12,\n",
    "    \"dropout\": 0.2,\n",
    "}\n",
    "model = DenseNetLit(hyperparameters=hyperparameters)\n",
    "logger = MLFlowLogger(experiment_name=\"Densenet-EuroSat\", save_dir=\"mlruns\")\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_acc\",\n",
    "            mode=\"max\",\n",
    "            dirpath=\"checkpoints/densenet-eurosat\",\n",
    "            filename=\"{epoch:02d}-{val_acc:.3f}\",\n",
    "        )\n",
    "    ],\n",
    "    precision=\"16-mixed\",\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d85e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt = trainer.checkpoint_callback.best_model_path\n",
    "trainer.test(model, datamodule=data, ckpt_path=best_ckpt)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6191e39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

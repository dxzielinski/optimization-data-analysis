{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from wide_resnet import WideResnetLit\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "\n",
    "L.seed_everything(42)\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda7686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]\n",
    "# source: https://pytorch.org/vision/stable/transforms.html\n",
    "transforms_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "transforms_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transforms_train\n",
    ")\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transforms_test\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=512, num_workers=31, persistent_workers=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=512, num_workers=31, persistent_workers=True\n",
    ")\n",
    "indices = torch.randperm(len(train_dataset))[: int(0.2 * len(train_dataset))]\n",
    "validation_dataset = torch.utils.data.Subset(train_dataset, indices=indices)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dataset=validation_dataset, batch_size=512, num_workers=31, persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5f6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/dxzielinski/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/dxzielinski/Desktop/github-repositories/optimization-data-analysis/models/checkpoints/wide_resnet exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | WideResNet         | 7.4 M  | train\n",
      "1 | criterion | CrossEntropyLoss   | 0      | train\n",
      "2 | train_acc | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "7.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.4 M     Total params\n",
      "29.600    Total estimated model params size (MB)\n",
      "96        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 1/98 [00:01<02:05,  0.77it/s, v_num=55a6, train/loss_step=7.790, train/acc_step=0.109]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxzielinski/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 98/98 [00:26<00:00,  3.77it/s, v_num=55a6, train/loss_step=2.230, train/acc_step=0.238, val/loss=2.110, val/acc=0.194, train/loss_epoch=5.440, train/acc_epoch=0.157]"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`ModelCheckpoint(monitor='val_f1_macro')` could not find the monitored key in the returned metrics: ['train/loss', 'train/loss_step', 'train/acc', 'train/acc_step', 'val/loss', 'val/acc', 'train/loss_epoch', 'train/acc_epoch', 'epoch', 'step']. HINT: Did you call `log('val_f1_macro', value)` in the `LightningModule`?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMisconfigurationException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     16\u001b[39m logger = MLFlowLogger(experiment_name=\u001b[33m\"\u001b[39m\u001b[33mWideResnet\u001b[39m\u001b[33m\"\u001b[39m, save_dir=\u001b[33m\"\u001b[39m\u001b[33mmlruns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m trainer = L.Trainer(\n\u001b[32m     18\u001b[39m     max_epochs=\u001b[32m50\u001b[39m,\n\u001b[32m     19\u001b[39m     logger=logger,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     num_sanity_val_steps=\u001b[32m0\u001b[39m,\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:217\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m    216\u001b[39m     \u001b[38;5;28mself\u001b[39m.advance()\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:470\u001b[39m, in \u001b[36m_FitLoop.on_advance_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    468\u001b[39m call._call_callback_hooks(trainer, \u001b[33m\"\u001b[39m\u001b[33mon_train_epoch_end\u001b[39m\u001b[33m\"\u001b[39m, monitoring_callbacks=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    469\u001b[39m call._call_lightning_module_hook(trainer, \u001b[33m\"\u001b[39m\u001b[33mon_train_epoch_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_train_epoch_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitoring_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m trainer._logger_connector.on_epoch_end()\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.restarting \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epoch_loop._num_ready_batches_reached():\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# since metric-based schedulers require access to metrics and those are not currently saved in the\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# checkpoint, the plateau schedulers shouldn't be updated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:227\u001b[39m, in \u001b[36m_call_callback_hooks\u001b[39m\u001b[34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[32m    226\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback.state_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    231\u001b[39m     pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:325\u001b[39m, in \u001b[36mModelCheckpoint.on_train_epoch_end\u001b[39m\u001b[34m(self, trainer, pl_module)\u001b[39m\n\u001b[32m    323\u001b[39m monitor_candidates = \u001b[38;5;28mself\u001b[39m._monitor_candidates(trainer)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._every_n_epochs >= \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (trainer.current_epoch + \u001b[32m1\u001b[39m) % \u001b[38;5;28mself\u001b[39m._every_n_epochs == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_topk_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[38;5;28mself\u001b[39m._save_last_checkpoint(trainer, monitor_candidates)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/github-repositories/optimization-data-analysis/.venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:383\u001b[39m, in \u001b[36mModelCheckpoint._save_topk_checkpoint\u001b[39m\u001b[34m(self, trainer, monitor_candidates)\u001b[39m\n\u001b[32m    377\u001b[39m     m = (\n\u001b[32m    378\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`ModelCheckpoint(monitor=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.monitor\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m)` could not find the monitored key in the returned\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    379\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m metrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(monitor_candidates)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m HINT: Did you call `log(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.monitor\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m, value)` in the `LightningModule`?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    381\u001b[39m     )\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.fit_loop.epoch_loop.val_loop._has_run:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(m)\n\u001b[32m    384\u001b[39m     warning_cache.warn(m)\n\u001b[32m    385\u001b[39m \u001b[38;5;28mself\u001b[39m._save_monitor_checkpoint(trainer, monitor_candidates)\n",
      "\u001b[31mMisconfigurationException\u001b[39m: `ModelCheckpoint(monitor='val_f1_macro')` could not find the monitored key in the returned metrics: ['train/loss', 'train/loss_step', 'train/acc', 'train/acc_step', 'val/loss', 'val/acc', 'train/loss_epoch', 'train/acc_epoch', 'epoch', 'step']. HINT: Did you call `log('val_f1_macro', value)` in the `LightningModule`?"
     ]
    }
   ],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(self, train_loader, validation_loader):\n",
    "        super().__init__()\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.validation_loader\n",
    "\n",
    "\n",
    "data = DataModule(train_loader, validation_loader)\n",
    "model = WideResnetLit(depth=32, weight_decay=10e-4)\n",
    "logger = MLFlowLogger(experiment_name=\"WideResnet\", save_dir=\"mlruns\")\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    logger=logger,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            monitor=\"val_acc\",\n",
    "            mode=\"max\",\n",
    "            dirpath=\"checkpoints/wide_resnet\",\n",
    "            filename=\"{epoch:02d}-{val_acc:.3f}\",\n",
    "        )\n",
    "    ],\n",
    "    precision=\"16-mixed\",\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59599fa8",
   "metadata": {},
   "source": [
    "Val F1 macro is 0.799\n",
    "\n",
    "Val accuracy is 0.798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt = trainer.checkpoint_callback.best_model_path\n",
    "print(\"Best checkpoint path:\", best_ckpt)\n",
    "trainer.test(model, datamodule=data, ckpt_path=best_ckpt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
